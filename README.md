# Agentic ES Ingestion Pipeline ğŸš€

[![Python 3.9+](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/)
[![Elasticsearch](https://img.shields.io/badge/DB-Elasticsearch-005571?logo=elasticsearch)](https://www.elastic.co/)
[![LangGraph](https://img.shields.io/badge/Orchestration-LangGraph-orange)](https://github.com/langchain-ai/langgraph)

## ğŸ“– Overview

An advanced, agentic AIâ€“driven data ingestion pipeline designed to transform diverse, non-standardized vendor data (primarily CSVs) into rigid, production-grade Elasticsearch JSON schemas. 

By leveraging **LangGraph** for orchestration and **GPT-5-mini** (via OpenAI) for reasoning, this system eliminates the need for manual, vendor-specific ETL scripts. It dynamically discovers schemas, generates transformation logic, and validates results against a strict central contract.

## âœ¨ Key Strategic Advantages

- ğŸ¤– **Zero Engineering Effort**: Onboard new vendors without writing a single line of procedural code.
- ğŸ§  **Dynamic Orchestration**: Uses iterative state-machine logic (LangGraph) to handle complex, non-linear transformation tasks.
- ğŸ›¡ï¸ **Schema-First Contract**: Guarantees Elasticsearch compatibility by enforcing schema definitions at the edge.
- ğŸ“ˆ **Scalable Execution**: Combines LLM-driven planning with deterministic Python execution for high-performance data processing.

## ğŸ› ï¸ Technology Stack

- **Orchestration**: LangGraph
- **AI Core**: OpenAI GPT-5-mini (Agentic Reasoning)
- **Data Processing**: Pandas / Python
- **Search Engine**: Elasticsearch (ES)
- **Observability**: Slack Integration (Real-time pipeline status)

## ğŸ—ï¸ Pipeline Architecture

```mermaid
graph TD
    S1[Start: Read Raw CSV] --> S2[Validate Raw Data]
    S2 --> S3[Agentic Conversion Logic]
    subgraph "Agentic Reasoning Layer"
        S3-1[Profiler Agent] --> S3-2[Planner Agent]
        S3-2 --> S3-3[Generator Agents]
    end
```

## ğŸš€ Pipeline Stages (S1-S8)

1.  **S1: Read**: Ingests raw vendor files from `data/raw_csvs/`.
2.  **S2: Validate Raw**: Initial sanity checks on the incoming data structure.
3.  **S3: Agentic Convert**: The core reasoning step where agents (Profiler, Planner, Generator) map raw data to the ES schema.
4.  **S4: Validate Final**: Ensures the transformed data perfectly matches the target ES index requirements.
5.  **S5: Pull ES**: Retrieves current state from Elasticsearch to handle updates and deduplication.
6.  **S6: Compare**: Performs a diff between incoming data and current ES state.
7.  **S7: Report**: Generates a detailed audit log of changes, successes, and failures.
8.  **S8: Push ES**: Performs bulk indexing into Elasticsearch.

## ğŸ“‚ Directory Structure

```text
.
â”œâ”€â”€ agents/             # AI Agents (Profiler, Planner, Generator)
â”œâ”€â”€ data/               
â”‚   â”œâ”€â”€ raw_csvs/       # Input vendor files
â”‚   â””â”€â”€ schema/         # ES target schemas
â”œâ”€â”€ executor/           # Deterministic code execution engines
â”œâ”€â”€ graph/              # LangGraph orchestration logic
â”œâ”€â”€ pipeline/           # Step-by-step pipeline modules (S1-S8)
â”œâ”€â”€ observability/      # Monitoring and alerts (Slack)
â”œâ”€â”€ config.py           # Configuration & Credentials
â”œâ”€â”€ main.py             # Entry point
â””â”€â”€ requirements.txt    # Dependencies
```

## âš™ï¸ Setup & Usage

1.  **Install Dependencies**:
    ```bash
    pip install -r requirements.txt
    ```

2.  **Configure Environment**:
    Update `config.py` with your OpenAI and Elasticsearch credentials.

3.  **Run the Pipeline**:
    ```bash
    python main.py
    ```
    *You will be prompted to enter the target index name (e.g., `products`, `vendors`).*
